{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET CREATION\n",
    "* Team: 1\n",
    "* Students: Magnin Jonathan, Nonaca Darja, Shmeis Zeinab, Wang Shu\n",
    "\n",
    "## LICENSE\n",
    "### YOLO LICENSE\n",
    "Version 2, July 29 2016\n",
    "\n",
    "THIS SOFTWARE LICENSE IS PROVIDED \"ALL CAPS\" SO THAT YOU KNOW IT IS SUPER\n",
    "SERIOUS AND YOU DON'T MESS AROUND WITH COPYRIGHT LAW BECAUSE YOU WILL GET IN\n",
    "TROUBLE HERE ARE SOME OTHER BUZZWORDS COMMONLY IN THESE THINGS WARRANTIES\n",
    "LIABILITY CONTRACT TORT LIABLE CLAIMS RESTRICTION MERCHANTABILITY. NOW HERE'S\n",
    "THE REAL LICENSE:\n",
    "\n",
    "0. Darknet is public domain.\n",
    "1. Do whatever you want with it.\n",
    "2. Stop emailing me about it!\n",
    "\n",
    "## Description\n",
    "This code uses raw data downloaded from (TODO : put link) to create a proper dataset in CSV files. Since we use both public and private data, only public data are uploaded on this git, which mean that a part of this code cannot work. That is why this code should *NOT BE EXECUTED* and is here only for reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"complete_data.csv\"\n",
    "test_filename = \"complete_test_data.csv\"\n",
    "generate_test_data = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example case - DO NOT USE -\n",
    "header = [\"student_name\",\"to_drop1\",\"sct_student\",\"course\",\"prof\",\"to_drop2\",\"course_sct\"];\n",
    "d12_fall_pd = pd.read_csv(\"data/data2012autumn.csv\",names=header)\n",
    "sct_split = d12_fall_pd[\"course_sct\"].str.split(\",\",n=1,expand=True)\n",
    "d12_fall_pd[\"course_sct\"] = sct_split[0]\n",
    "d12_fall_pd.insert(7,\"year\",sct_split[1])\n",
    "d12_fall_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general case, huge df with every semester concatenated - WHAT YOU WANT -\n",
    "# years sampled\n",
    "if generate_test_data :\n",
    "    years = [\"2017\", \"2018\", \"2019\"];\n",
    "else :\n",
    "    years = [\"2015\",\"2016\"];\n",
    "header = [\"student_name\",\"to_drop1\",\"sct_student\",\"course\",\"prof\",\"to_drop2\",\"course_sct\"];\n",
    "\n",
    "# all other df\n",
    "all_data = pd.DataFrame(columns=header)\n",
    "for year in years:\n",
    "    temp = pd.read_csv(\"data/data\" + year + \"autumn.csv\",names=header)\n",
    "    sct_split = temp[\"course_sct\"].str.split(\",\", n=1, expand=True)\n",
    "    temp[\"course_sct\"] = sct_split[0]\n",
    "    temp.insert(7, \"year\", sct_split[1])\n",
    "    all_data = pd.concat([all_data,temp], axis=0)\n",
    "    temp = pd.read_csv(\"data/data\" + year + \"spring.csv\",names=header)\n",
    "    sct_split = temp[\"course_sct\"].str.split(\",\",n=1,expand=True)\n",
    "    temp[\"course_sct\"] = sct_split[0]\n",
    "    temp.insert(7,\"year\",sct_split[1])\n",
    "    all_data = pd.concat([all_data,temp],axis=0)\n",
    "    \n",
    "# drop useless columns\n",
    "all_data = all_data.drop([\"to_drop1\", \"to_drop2\"], axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.iloc[100000] # just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lxml # in case of error lxml not found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing the xml files in data_login to create features.csv\n",
    "#this block takes time to run, we need to run it only once to generate features.csv file\n",
    "import os\n",
    "import csv\n",
    "\n",
    "pathName = os.getcwd() + '/data_login'\n",
    "fileNames = os.listdir(pathName)             #put all the files names into a list\n",
    "#column_list = list(df_login.columns)         #get all the colmn names\n",
    "col = ['gender', 'student_name', 'nationality'] #select the feature columns\n",
    "\n",
    "with open('features.csv', 'w', newline='') as csvfile: #open the file where to write\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=col)                       #initialize the writer\n",
    "    writer.writeheader()\n",
    "    for filexls in fileNames:  #iterate through all .xml files \n",
    "        df_new = pd.read_html(os.path.join(pathName, filexls), header = 2) # read .xml\n",
    "        if df_new != []:\n",
    "            df_new = df_new[0]\n",
    "            for index, row in df_new.iterrows():  #write the content of a non-empty .xls into an features.csv\n",
    "                writer.writerow({'gender':str(row['Civilité']) , 'student_name':str(row['Nom Prénom']) , 'nationality':str(row['Nationalité'])})\n",
    "                \n",
    "os.system('sed -i -e s/\\xa0/\\x20/g features.csv') # replace \"space\" special character by \"space\" normal character\n",
    "os.system('sed -i \\'\\' \\'s/nationality /nationality/g\\' features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features.csv', sep=',', header=0, encoding='latin1')\n",
    "#for idx, row in features.iterrows():\n",
    "#      features.loc[idx, 'student_name'] =features.loc[idx, 'Nom Prénom'].replace('\\xa0', ' ')\n",
    "#data_merged = all_data.merge(features, left_on ='student_name', right_on = 'Nom Prénom',  how = 'inner')\n",
    "#we need to remove \\xa0 from the names in the features before to merge\n",
    "#also let's do unique the features rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.loc[1, 'student_name'] #debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop_duplicates(inplace=True)\n",
    "features['student_id'] = features.index\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = all_data.merge(features, left_on = \"student_name\", right_on = \"student_name\", how = \"inner\")\n",
    "complete_data.drop_duplicates(inplace=True)\n",
    "complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data[complete_data[\"student_name\"] == \"Magnin Jonathan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.drop([\"student_name\"], axis=1, inplace=True)\n",
    "complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.to_csv('./complete_test_data.csv', sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
